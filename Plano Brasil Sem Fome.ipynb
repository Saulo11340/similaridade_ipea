{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12eb542-afd0-4530-b5a7-dae3e0656a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando os datasets...\n",
      "Datasets carregados com sucesso.\n",
      "Normalizando e combinando os textos das políticas e ações...\n",
      "Textos normalizados e combinados.\n",
      "Calculando embeddings das políticas...\n",
      "Embeddings das políticas calculados.\n",
      "Calculando embeddings das ações...\n",
      "Embeddings das ações calculados.\n",
      "Iniciando cálculo de similaridades...\n",
      "Processando lote de políticas 0 até 1...\n",
      "Criando dataframe de resultados...\n",
      "Ordenando resultados por similaridade...\n",
      "Exportando resultados para arquivo CSV...\n",
      "Processamento concluído. Arquivo 'saída_correspondencias.csv' gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Inicialização do modelo e tokenizer\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Função para normalização de texto\n",
    "def normalize_text(text):\n",
    "    import unicodedata\n",
    "    import re\n",
    "    if pd.isnull(text): return \"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "# Função para truncar texto usando o tokenizer\n",
    "def truncate_text(text, max_tokens=512):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens, add_special_tokens=True)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "# Função para cálculo de embeddings\n",
    "def get_embeddings(text):\n",
    "    # Trunca o texto antes de passar ao modelo\n",
    "    truncated_text = truncate_text(text)\n",
    "    inputs = tokenizer(truncated_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Usa apenas o primeiro token da sequência (CLS token) como representação\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Função para cálculo de similaridade\n",
    "def calculate_similarity_matrix(embeddings_politicas, embeddings_acoes):\n",
    "    # Calcula similaridades de forma vetorizada\n",
    "    return cosine_similarity(embeddings_politicas, embeddings_acoes)\n",
    "\n",
    "# Carregamento dos datasets\n",
    "print(\"Carregando os datasets...\")\n",
    "politicas = pd.read_csv('Plano_Brasil_Sem_Fome.csv', encoding='utf-8')\n",
    "acoes = pd.read_csv('acoes_assistencia.csv', encoding='utf-8')\n",
    "print(\"Datasets carregados com sucesso.\")\n",
    "\n",
    "# Normalização e combinação de textos\n",
    "print(\"Normalizando e combinando os textos das políticas e ações...\")\n",
    "politicas['texto_concat'] = (\n",
    "    politicas['nome'] * 5 + ' ' +\n",
    "    politicas['publico_alvo_nome'] * 4 + ' ' +\n",
    "    politicas['publico_alvo_legislacao'] * 3 + ' ' +\n",
    "    politicas['objetivos'] * 2 + ' ' +\n",
    "    politicas['area'] + ' ' +\n",
    "    politicas['cat_nome']\n",
    ").apply(normalize_text)\n",
    "\n",
    "acoes['texto_concat'] = (\n",
    "    acoes['Título'] * 5 + ' ' +\n",
    "    acoes['Descrição'] * 4 + ' ' +\n",
    "    acoes['Produto'] * 3 + ' ' +\n",
    "    acoes['Especificação do Produto'] * 2 + ' ' +\n",
    "    acoes['Beneficiário']\n",
    ").apply(normalize_text)\n",
    "print(\"Textos normalizados e combinados.\")\n",
    "\n",
    "# Cálculo dos embeddings\n",
    "print(\"Calculando embeddings das políticas...\")\n",
    "embeddings_politicas = np.stack(politicas['texto_concat'].apply(get_embeddings))\n",
    "print(\"Embeddings das políticas calculados.\")\n",
    "\n",
    "print(\"Calculando embeddings das ações...\")\n",
    "embeddings_acoes = np.stack(acoes['texto_concat'].apply(get_embeddings))\n",
    "print(\"Embeddings das ações calculados.\")\n",
    "\n",
    "# Processamento em lotes para cálculo de similaridades\n",
    "print(\"Iniciando cálculo de similaridades...\")\n",
    "batch_size = 50  # Ajuste para sua memória disponível\n",
    "results = []\n",
    "\n",
    "for start_idx in range(0, len(politicas), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(politicas))\n",
    "    print(f\"Processando lote de políticas {start_idx} até {end_idx}...\")\n",
    "\n",
    "    # Extrai o lote de embeddings\n",
    "    embeddings_batch = embeddings_politicas[start_idx:end_idx]\n",
    "\n",
    "    # Calcula a similaridade do lote com todas as ações\n",
    "    similarity_matrix = calculate_similarity_matrix(embeddings_batch, embeddings_acoes)\n",
    "\n",
    "    # Adiciona os resultados\n",
    "    for i, policy_idx in enumerate(range(start_idx, end_idx)):\n",
    "        policy_id = politicas.iloc[policy_idx]['id']\n",
    "        policy_name = politicas.iloc[policy_idx]['nome']\n",
    "\n",
    "        for j, action_idx in enumerate(range(len(acoes))):\n",
    "            action_id = acoes.iloc[action_idx]['Ação']\n",
    "            action_title = acoes.iloc[action_idx]['Título']\n",
    "            action_uo = acoes.iloc[action_idx]['UO']\n",
    "            action_function = acoes.iloc[action_idx]['Função']\n",
    "            similarity = similarity_matrix[i, j]\n",
    "\n",
    "            results.append({\n",
    "                'ID da política': policy_id,\n",
    "                'Nome da política': policy_name,\n",
    "                'ID da ação': action_id,\n",
    "                'Título da ação': action_title,\n",
    "                'UO responsável': action_uo,\n",
    "                'Função relacionada': action_function,\n",
    "                'Similaridade': similarity\n",
    "            })\n",
    "\n",
    "# Criação do dataframe de resultados\n",
    "print(\"Criando dataframe de resultados...\")\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ordenação dos resultados por similaridade (decrescente)\n",
    "print(\"Ordenando resultados por similaridade...\")\n",
    "results_df = results_df.sort_values(by='Similaridade', ascending=False)\n",
    "\n",
    "# Exportação dos resultados para um arquivo CSV\n",
    "print(\"Exportando resultados para arquivo CSV...\")\n",
    "results_df.to_csv('saída_correspondencias.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Processamento concluído. Arquivo 'saída_correspondencias.csv' gerado com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31687512-0e8c-4179-9a17-cd521a139247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
